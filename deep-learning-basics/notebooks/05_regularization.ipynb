{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Closing the Train/Val Gap\n",
    "\n",
    "In notebook 03, our `SimpleCNN` reached ~70% validation accuracy on CIFAR-10 — but training accuracy climbed to ~75% and was still rising. That gap between training and validation performance is **overfitting**: the model memorises training data instead of learning general patterns.\n",
    "\n",
    "**Regularization** is any technique that reduces this gap by constraining the model. This notebook compares four common approaches:\n",
    "\n",
    "1. **Data augmentation** — show the model transformed versions of each image\n",
    "2. **Dropout** — randomly zero out activations during training\n",
    "3. **Weight decay (L2)** — penalise large weights in the optimizer\n",
    "4. **Batch normalization** — normalize activations within each mini-batch\n",
    "\n",
    "We keep everything else fixed (same architecture, optimizer, learning rate, epochs) so we can isolate the effect of each technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dlbasics import set_seed, plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Same CIFAR-10 setup as notebook 03: normalize with per-channel statistics, split 50k training images into 45k train / 5k validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Standard transform (no augmentation)\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# Augmented transform (used in sections 4 and 8)\n",
    "aug_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data\"\n",
    "\n",
    "train_full = datasets.CIFAR10(root=data_root, train=True, download=True, transform=base_transform)\n",
    "test_ds = datasets.CIFAR10(root=data_root, train=False, download=True, transform=base_transform)\n",
    "\n",
    "val_size = 5000\n",
    "train_size = len(train_full) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(\n",
    "    train_full,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def make_loaders(train_dataset, val_dataset):\n",
    "    \"\"\"Create train and val DataLoaders with consistent settings.\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "train_loader, val_loader = make_loaders(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "\n",
    "Same `train_one_epoch`, `evaluate`, and `fit` functions from notebook 03 (copied inline to keep this notebook self-contained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Run one full pass over the training set, updating parameters after each batch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total_samples += xb.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model on a dataset without computing gradients.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total_samples += xb.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs=5):\n",
    "    \"\"\"Train for multiple epochs, returning a history dict with loss and accuracy curves.\"\"\"\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Baseline CNN (no regularization)\n",
    "\n",
    "Same `SimpleCNN` from notebook 03. This is our control: we expect to see train accuracy climb higher than val accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "baseline = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(baseline.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=== Baseline (no regularization) ===\")\n",
    "history_baseline = fit(baseline, train_loader, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_baseline, \"Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gap between the train and val curves is the overfitting we want to reduce. Training loss keeps dropping while validation loss flattens or rises — the model is memorising instead of generalising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — Data Augmentation\n",
    "\n",
    "The cheapest regularizer: show the model randomly transformed versions of each training image. We use two standard augmentations for CIFAR-10:\n",
    "\n",
    "- `RandomHorizontalFlip()` — 50% chance to flip left/right\n",
    "- `RandomCrop(32, padding=4)` — pad 4 pixels on each side, then crop back to 32x32 (small random shifts)\n",
    "\n",
    "This effectively increases dataset diversity without collecting new data. The model sees a slightly different image each epoch, making it harder to memorise specific pixel patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload training data with augmentation applied\n",
    "train_full_aug = datasets.CIFAR10(root=data_root, train=True, download=True, transform=aug_transform)\n",
    "\n",
    "# Use the same indices for the train/val split\n",
    "train_ds_aug = torch.utils.data.Subset(train_full_aug, train_ds.indices)\n",
    "\n",
    "train_loader_aug, _ = make_loaders(train_ds_aug, val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model_aug = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_aug.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=== Data Augmentation ===\")\n",
    "history_aug = fit(model_aug, train_loader_aug, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Dropout\n",
    "\n",
    "Dropout randomly sets a fraction of activations to zero during training. This prevents co-adaptation: neurons can't rely on specific other neurons always being present, so each one must learn more robust features.\n",
    "\n",
    "- `nn.Dropout2d(p)` after conv blocks — drops entire feature maps (better for spatial data than per-element dropout)\n",
    "- `nn.Dropout(p)` before the classifier — standard element-wise dropout for the fully connected layer\n",
    "\n",
    "At test time, dropout is turned off and activations are scaled to compensate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithDropout(nn.Module):\n",
    "    def __init__(self, num_classes=10, drop_conv=0.25, drop_fc=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(drop_conv),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(drop_conv),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(drop_fc),\n",
    "            nn.Linear(64 * 8 * 8, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model_drop = CNNWithDropout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_drop.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=== Dropout ===\")\n",
    "history_drop = fit(model_drop, train_loader, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Weight Decay (L2 Regularization)\n",
    "\n",
    "Weight decay adds a penalty proportional to the squared magnitude of the weights to the loss:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\frac{\\lambda}{2} \\sum_i w_i^2$$\n",
    "\n",
    "This discourages any single weight from growing too large. In practice, we don't modify the loss — PyTorch's optimizer applies the penalty directly during the weight update step via the `weight_decay` parameter.\n",
    "\n",
    "Same `SimpleCNN`, same data, just `weight_decay=1e-4` in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model_wd = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_wd.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(\"=== Weight Decay (L2) ===\")\n",
    "history_wd = fit(model_wd, train_loader, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Batch Normalization\n",
    "\n",
    "Batch normalization normalizes activations within each mini-batch to have zero mean and unit variance, then applies a learnable affine transform. It was introduced primarily to stabilize and speed up training, but it also has a mild regularization effect because the batch statistics introduce noise.\n",
    "\n",
    "We add `nn.BatchNorm2d` after each convolutional layer, before ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model_bn = CNNWithBatchNorm().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_bn.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"=== Batch Normalization ===\")\n",
    "history_bn = fit(model_bn, train_loader, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — Combined: All Techniques Together\n",
    "\n",
    "Now we combine everything: data augmentation + dropout + weight decay + batch normalization. This should give the best generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, drop_conv=0.25, drop_fc=0.5):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(drop_conv),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout2d(drop_conv),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(drop_fc),\n",
    "            nn.Linear(64 * 8 * 8, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model_combined = RegularizedCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_combined.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(\"=== Combined (aug + dropout + weight decay + batch norm) ===\")\n",
    "history_combined = fit(model_combined, train_loader_aug, val_loader, criterion, optimizer, device, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Side-by-side results for all six runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Baseline\":       history_baseline,\n",
    "    \"Data Aug\":       history_aug,\n",
    "    \"Dropout\":        history_drop,\n",
    "    \"Weight Decay\":   history_wd,\n",
    "    \"Batch Norm\":     history_bn,\n",
    "    \"Combined\":       history_combined,\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"{'Method':<16} {'Train Acc':>10} {'Val Acc':>10} {'Train Loss':>11} {'Val Loss':>10} {'Gap':>8}\")\n",
    "print(\"-\" * 67)\n",
    "for name, h in results.items():\n",
    "    ta = h[\"train_acc\"][-1]\n",
    "    va = h[\"val_acc\"][-1]\n",
    "    tl = h[\"train_loss\"][-1]\n",
    "    vl = h[\"val_loss\"][-1]\n",
    "    gap = ta - va\n",
    "    print(f\"{name:<16} {ta:>10.4f} {va:>10.4f} {tl:>11.4f} {vl:>10.4f} {gap:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: final validation accuracy for each method\n",
    "names = list(results.keys())\n",
    "val_accs = [h[\"val_acc\"][-1] for h in results.values()]\n",
    "train_accs = [h[\"train_acc\"][-1] for h in results.values()]\n",
    "\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars_train = ax.bar(x - width/2, train_accs, width, label=\"Train\", color=\"#4c72b0\")\n",
    "bars_val = ax.bar(x + width/2, val_accs, width, label=\"Val\", color=\"#dd8452\")\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Train vs Val Accuracy by Regularization Method\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, rotation=15, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0.4, 0.85)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars_train:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{bar.get_height():.1%}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "for bar in bars_val:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{bar.get_height():.1%}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy curves for all methods\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = list(range(1, 6))\n",
    "for name, h in results.items():\n",
    "    ax1.plot(epochs, h[\"val_acc\"], marker=\"o\", label=name)\n",
    "    ax2.plot(epochs, h[\"val_loss\"], marker=\"o\", label=name)\n",
    "\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Validation Accuracy\")\n",
    "ax1.set_title(\"Validation Accuracy\")\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Validation Loss\")\n",
    "ax2.set_title(\"Validation Loss\")\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- **Data augmentation** is one of the cheapest and most effective regularizers — it adds no parameters and usually improves generalization significantly. For image tasks, `RandomHorizontalFlip` + `RandomCrop` are strong baselines.\n",
    "- **Dropout** reduces co-adaptation between neurons. `Dropout2d` is often preferred in convolutional layers since it drops entire feature maps rather than individual pixels.\n",
    "- **Weight decay (L2 regularization)** penalizes large weights, encouraging simpler solutions. It’s a single hyperparameter (`weight_decay`) with no architectural changes required (**AdamW** is the standard choice).\n",
    "- **Batch normalization** was designed to stabilize training (Ioffe & Szegedy, 2015), and the stochasticity of batch statistics provides mild regularization.\n",
    "- **Combining techniques** often works best since they address different failure modes (memorization, co-adaptation, weight growth, unstable optimization).\n",
    "- The train/val **generalization gap** is as important as raw accuracy. A method that slightly lowers training accuracy but closes the gap is often improving generalization.\n",
    "- **Not covered here**: early stopping, learning rate scheduling, and stronger augmentation policies (Cutout, Mixup, AutoAugment).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
